{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Concatenate,Dense\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import tqdm\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n",
      "[-1. -1. -1. -1.]\n",
      "24 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "print(state_dim[0],action_dim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> List[np.ndarray]:\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (\n",
    "        state.astype(np.float64),\n",
    "        np.array(reward, np.float32),\n",
    "        np.array(done, np.int32)\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor):\n",
    "    return tf.numpy_function(\n",
    "        env_step, [action], [tf.float64, tf.float32, tf.int32]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_weight_init = tf.keras.initializers.RandomUniform(minval=-.003, maxval=.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = tf.constant(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func approx for deterministic policy \n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,action_shape):\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1 = Dense(400,activation='relu')\n",
    "        self.fc2 = Dense(300,activation='relu')\n",
    "        #fed through tanh to bound actions between (-1,1) \n",
    "        self.fc3 = Dense(action_shape,activation='tanh',kernel_initializer=last_weight_init)\n",
    "    def call(self,inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x * bound\n",
    "    \n",
    "# func approx for Q(s,a)\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,action_shape):\n",
    "        super(Critic,self).__init__()\n",
    "        self.state_fc1 = Dense(400,activation='relu')\n",
    "        self.state_fc2 = Dense(300,activation='relu')\n",
    "        \n",
    "        self.action_fc1 = Dense(400,activation='relu')\n",
    "        \n",
    "        self.concat = Concatenate()\n",
    "\n",
    "        self.out = Dense(1,kernel_initializer=last_weight_init)\n",
    "\n",
    "    def call(self,inputs,training):\n",
    "        [state,action] = inputs\n",
    "        state_x = self.state_fc1(state)\n",
    "        state_x = self.state_fc2(state_x)\n",
    "        \n",
    "        action_x = self.action_fc1(action)\n",
    "        \n",
    "        concat_x = self.concat([state_x,action_x])\n",
    "        \n",
    "        return self.out(concat_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_spec =  (\n",
    "    tf.TensorSpec([state_dim[0]], tf.float64, 'state'),\n",
    "    tf.TensorSpec([action_dim[0]], tf.float32, 'action'),\n",
    "    tf.TensorSpec([1], tf.float32, 'reward'),\n",
    "    tf.TensorSpec([state_dim[0]], tf.float64, 'next_state'),\n",
    "    tf.TensorSpec([1], tf.int32, 'done'),\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "max_length = int(1e6)\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length)\n",
    "replay_buffer.num_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(env.action_space.shape[0])\n",
    "critic = Critic(env.action_space.shape[0])\n",
    "\n",
    "actor_t_net = Actor(env.action_space.shape[0])\n",
    "critic_t_net = Critic(env.action_space.shape[0])\n",
    "\n",
    "actor_t_net.set_weights(actor.get_weights())\n",
    "critic_t_net.set_weights(critic.get_weights())\n",
    "\n",
    "max_episodes = 100\n",
    "gamma = .99\n",
    "max_steps = 10000\n",
    "actor_lr = .0001\n",
    "critic_lr = .0001\n",
    "\n",
    "actor_optim = Adam(learning_rate=actor_lr)\n",
    "critic_optim = Adam(learning_rate=critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tf.constant(env.reset(),dtype=tf.float64)\n",
    "transition\n",
    "while True:\n",
    "    action = actor(tf.expand_dims(state,0))\n",
    "    next_state,reward,done = tf_env_step(tf.squeeze(action))\n",
    "    transition = (state,tf.squeeze(action),tf.expand_dims(reward,0),next_state,tf.expand_dims(done,0))\n",
    "    transition = tf.nest.map_structure(lambda t: tf.stack([t] * 1),\n",
    "                                       transition)\n",
    "    replay_buffer.add_batch(transition)\n",
    "#     env.render()\n",
    "    if tf.cast(done,tf.bool):\n",
    "        break\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n",
    "replay_buffer.add_batch(transition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = replay_buffer.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(initial_state: tf.Tensor, gamma: float, max_steps: int, batch_size: int) -> tf.Tensor:\n",
    "    \n",
    "    state = initial_state\n",
    "    state_shape = initial_state.shape\n",
    "    \n",
    "    reward_shape = (1,)\n",
    "    episode_reward = tf.constant([0],dtype=tf.float32)\n",
    "    episode_reward.set_shape(reward_shape)\n",
    "    print(episode_reward.shape)\n",
    "    for t in tf.range(max_steps):\n",
    "        action = actor(tf.expand_dims(state,0))\n",
    "        next_state,reward,done = tf_env_step(tf.squeeze(action))\n",
    "        \n",
    "        next_state.set_shape(state_shape)\n",
    "        reward.set_shape(reward_shape)\n",
    "        done.set_shape(reward_shape)\n",
    "\n",
    "        transition = (state,tf.squeeze(action),reward,next_state,done)\n",
    "        transition = tf.nest.map_structure(lambda t: tf.stack([t] * 1),\n",
    "                                       transition)\n",
    "        \n",
    "        transitions_stored += 1\n",
    "        replay_buffer.add_batch(transition)\n",
    "        \n",
    "        if transitions_stored >= tf.constant(batch_size):\n",
    "            sample = replay_buffer.as_dataset(sample_batch_size=1,num_steps=1)\n",
    "            iterator = iter(sample)\n",
    "            (states,actions,rewards,next_states,dones),_ = iterator.next()\n",
    "            with tf.GradientTape() as tape:\n",
    "                td_target = rewards + gamma * critic_t_net([next_states,actions])\n",
    "                td_pred = critic([states,actions])\n",
    "                critic_loss = tf.reduce_mean(tf.math.square(td_pred-td_target))\n",
    "            critic_grads = tape.gradient(critic_loss,critic.trainable_variables)\n",
    "            critic_optim.apply_gradients(zip(critic_grads,critic.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                actions = actor(states)\n",
    "                action_values = critic([states,actions])\n",
    "                actor_loss = -tf.math.reduce_mean(action_values)\n",
    "            actor_grads = tape.gradient(actor_loss,actor.trainable_variables)\n",
    "            actor_optim.apply_gradients(zip(actor_grads,actor.trainable_variables))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        episode_reward += reward \n",
    "        episode_reward.set_shape(reward_shape)\n",
    "    \n",
    "        if tf.cast(done,tf.bool):\n",
    "            break\n",
    "\n",
    "    return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [1], indices.shape [1], params.shape [1000000,1]\n\t [[{{node while/body/_1/while/TFUniformReplayBuffer/ResourceScatterUpdate_3}}]] [Op:__inference_train_step_1110255]\n\nFunction call stack:\ntrain_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-517-6451731e769b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [1], indices.shape [1], params.shape [1000000,1]\n\t [[{{node while/body/_1/while/TFUniformReplayBuffer/ResourceScatterUpdate_3}}]] [Op:__inference_train_step_1110255]\n\nFunction call stack:\ntrain_step\n"
     ]
    }
   ],
   "source": [
    "running_reward = 0\n",
    "reward_threshold = 195\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        state = tf.constant(env.reset(),dtype=tf.float64)\n",
    "        episode_reward = float(train_step(state,gamma,max_steps,64))\n",
    "                \n",
    "        running_reward = .99 * running_reward + .01 * episode_reward\n",
    "    \n",
    "        t.set_description(f\"Episode {i}\")\n",
    "        t.set_postfix(episode_reward=episode_reward,running_reward=running_reward)\n",
    "        \n",
    "        if running_reward > reward_threshold:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
